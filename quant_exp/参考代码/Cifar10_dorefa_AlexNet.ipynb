{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cifar10-dorefa-AlexNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5T_rAWzsIjD",
        "outputId": "d919aab9-6731-485a-bbf7-1f337941be8c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abRcxH13uJf8",
        "outputId": "def5c0d5-7c46-4033-abc8-9296da69b04d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jun 29 07:15:31 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    25W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpH-QwVmMYOC"
      },
      "source": [
        "Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUVfYUafoAkY",
        "outputId": "a4095fe7-1edc-40cf-948a-1b7c5c839afb"
      },
      "source": [
        "from gdrive.MyDrive.utils.cifar10_prepare import cifar10_data_loader\n",
        "PATH = './content/gdrive/MyDrive/CIFAR10_dataset/data'\n",
        "batch_size_train,batch_size_test = 64,512\n",
        "train_loader,test_loader,classes = cifar10_data_loader(PATH,batch_size_train,batch_size_test)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "29DdSypFjo3o",
        "outputId": "1368ffc2-b7a5-44a9-ea49-8b5dd0dbefbb"
      },
      "source": [
        "'''\n",
        "现在的版本是全精度模型和量化模型的对比\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Function\n",
        "\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# make_hist_plot = False\n",
        "\n",
        "#调整不同超参数，记录不同曲线\n",
        "train_counters_log = []\n",
        "train_losses_log = []\n",
        "test_accuracy_log = []\n",
        "test_counter_log = []\n",
        "hps = []\n",
        "bitGs = [16,16]\n",
        "log_test = False #是否每隔一定间隔去测试一下？\n",
        "jy_log_interval = 1024 #每512个样本记录画图\n",
        "\n",
        "for bitG in bitGs:\n",
        "  n_epochs = 10\n",
        "  # bitG = 2\n",
        "  bitA = 16\n",
        "  bitW = 8\n",
        "\n",
        "  hps.append(f'bitG={bitG}')\n",
        "\n",
        "  hp_log = f'\\nn_epochs = {n_epochs}\\nbitG = {bitG}\\nbitA = {bitA}\\nbitW = {bitW}\\nbatch_size_train = {batch_size_train}\\n'\n",
        "  print(hp_log)\n",
        "\n",
        "  learning_rate = 0.01\n",
        "  momentum = 0.5\n",
        "  log_interval = 10\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  # 设置初始值种子，使得每次开始训练的初始值相同、结果可以复现\n",
        "  random_seed = 1\n",
        "  torch.manual_seed(random_seed)\n",
        "    if device == torch.device('cuda'):\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "  \n",
        "\n",
        "\n",
        "  \n",
        "  network = AlexNet(isquant = False).to(device)\n",
        "  # network = QuantNet()\n",
        "  # network = JustQuantGrad()\n",
        "  # optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
        "  #                       momentum=momentum)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(network.parameters())\n",
        "\n",
        "  # writer = SummaryWriter('runs/MNIST_tensorboard')\n",
        "  # images,labels=next(iter(train_loader))\n",
        "  # grid=torchvision.utils.make_grid(images)\n",
        "  # writer.add_image('images', grid)\n",
        "  # writer.add_graph(model=network,input_to_model=images)\n",
        "  # tb.close()\n",
        "\n",
        "\n",
        "  train_losses = []\n",
        "  train_counter = []\n",
        "\n",
        "  test_accuracy = []\n",
        "  test_counter = []\n",
        "\n",
        "  # test_losses = []\n",
        "  # test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
        "\n",
        "  def train(epoch):\n",
        "    network.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output = network(data)\n",
        "      # loss = F.nll_loss(output, target)\n",
        "      loss = criterion(output,target)\n",
        "      if math.isnan(loss.item()):\n",
        "        print(\"get nan loss!\")\n",
        "        network.load_state_dict(torch.load('./model.pth'))\n",
        "        break\n",
        "      torch.save(network.state_dict(), './model.pth')\n",
        "      torch.save(optimizer.state_dict(), './optimizer.pth')\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # 相等的样本间隔记录、画图\n",
        "      if (batch_size_train * batch_idx) % jy_log_interval == 0:\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "          train_losses.append(loss.item())\n",
        "          train_counter.append(\n",
        "              (batch_size_train * batch_idx) + ((epoch-1)*len(train_loader.dataset)))\n",
        "          if log_test == True:\n",
        "              tl,acc = test()\n",
        "              test_accuracy.append(acc.item())\n",
        "              test_counter.append(\n",
        "                  (batch_size_train * batch_idx) + ((epoch-1)*len(train_loader.dataset)))\n",
        "\n",
        "\n",
        "      # if batch_idx % log_interval == 0:\n",
        "      #   print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "      #     epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "      #     100. * batch_idx / len(train_loader), loss.item()))\n",
        "      #   train_losses.append(loss.item())\n",
        "      #   train_counter.append(\n",
        "      #     (batch_idx*batch_size_train) + ((epoch-1)*len(train_loader.dataset)))\n",
        "\n",
        "\n",
        "\n",
        "      #   writer.add_scalar('Train/Loss', loss.item(), (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "      #   writer.flush()\n",
        "\n",
        "  def test():\n",
        "    network.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        output = network(data)\n",
        "        test_loss += criterion(output, target).item()\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "  #   test_losses.append(test_loss)\n",
        "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "      test_loss, correct, len(test_loader.dataset),\n",
        "      100. * correct / len(test_loader.dataset)))\n",
        "    return test_loss,100. * correct / len(test_loader.dataset)\n",
        "\n",
        "\n",
        "\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    train(epoch)\n",
        "    \n",
        "    # writer.add_scalar('Train/Loss', loss.item(), (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "  #   writer.flush()\n",
        "    test_loss,accuracy = test()\n",
        "  # import matplotlib.pyplot as plt\n",
        "\n",
        "  train_counters_log.append(train_counter)\n",
        "  train_losses_log.append(train_losses)\n",
        "  test_counter_log.append(test_counter)\n",
        "  test_accuracy_log.append(test_accuracy)\n",
        "\n",
        "  result_log = f'\\ntrain_counter = {train_counter}\\ntrain_losses = {train_losses}'\n",
        "  \n",
        "  if log_test:\n",
        "    result_log = result_log + f'\\ntest_accuracy = {test_accuracy}'\n",
        "\n",
        "  file = open('run_results',mode='a+')\n",
        "  file.write(hp_log)\n",
        "  file.write(result_log)\n",
        "  file.close()\n",
        "\n",
        "# end hp explore\n",
        "\n",
        "fig = plt.figure(dpi=1000)\n",
        "for i in range(len(bitGs)):\n",
        "  \n",
        "  if log_test:\n",
        "    trl = fig.add_subplot(2,1,1) #训练误差\n",
        "    trl.plot(train_counters_log[i], train_losses_log[i])\n",
        "    trl.legend(hps,loc='upper right')\n",
        "    trl.set_xlabel('number of training examples')\n",
        "    trl.set_ylabel('cross entropy loss')\n",
        "\n",
        "    tea = fig.add_subplot(2,1,2) #测试精度\n",
        "    tea.plot(test_counter_log[i], test_accuracy_log[i])\n",
        "    tea.legend(hps,loc='upper right')\n",
        "    tea.set_xlabel('number of training examples')\n",
        "    tea.set_ylabel('test accuracy')\n",
        "  else:\n",
        "    plt.plot(train_counters_log[i], train_losses_log[i])\n",
        "    plt.legend(hps,loc='upper right')\n",
        "    plt.xlabel('number of training examples')\n",
        "    plt.ylabel('cross entropy loss')\n",
        "\n",
        "plt.savefig('exp')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "n_epochs = 3\n",
            "bitG = 16\n",
            "bitA = 32\n",
            "bitW = 32\n",
            "batch_size_train = 64\n",
            "\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.303882\n",
            "Train Epoch: 1 [1024/50000 (2%)]\tLoss: 2.304816\n",
            "Train Epoch: 1 [2048/50000 (4%)]\tLoss: 2.256161\n",
            "Train Epoch: 1 [3072/50000 (6%)]\tLoss: 2.201043\n",
            "Train Epoch: 1 [4096/50000 (8%)]\tLoss: 2.029397\n",
            "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 2.160360\n",
            "Train Epoch: 1 [6144/50000 (12%)]\tLoss: 2.005521\n",
            "Train Epoch: 1 [7168/50000 (14%)]\tLoss: 1.808043\n",
            "Train Epoch: 1 [8192/50000 (16%)]\tLoss: 1.840828\n",
            "Train Epoch: 1 [9216/50000 (18%)]\tLoss: 2.258432\n",
            "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 1.888587\n",
            "Train Epoch: 1 [11264/50000 (23%)]\tLoss: 1.870700\n",
            "Train Epoch: 1 [12288/50000 (25%)]\tLoss: 1.705283\n",
            "Train Epoch: 1 [13312/50000 (27%)]\tLoss: 1.875639\n",
            "Train Epoch: 1 [14336/50000 (29%)]\tLoss: 1.854397\n",
            "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 1.812885\n",
            "Train Epoch: 1 [16384/50000 (33%)]\tLoss: 1.945669\n",
            "Train Epoch: 1 [17408/50000 (35%)]\tLoss: 1.967223\n",
            "Train Epoch: 1 [18432/50000 (37%)]\tLoss: 1.786184\n",
            "Train Epoch: 1 [19456/50000 (39%)]\tLoss: 1.819039\n",
            "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 1.770605\n",
            "Train Epoch: 1 [21504/50000 (43%)]\tLoss: 1.890039\n",
            "Train Epoch: 1 [22528/50000 (45%)]\tLoss: 1.547214\n",
            "Train Epoch: 1 [23552/50000 (47%)]\tLoss: 1.648885\n",
            "Train Epoch: 1 [24576/50000 (49%)]\tLoss: 1.756006\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.734742\n",
            "Train Epoch: 1 [26624/50000 (53%)]\tLoss: 1.744550\n",
            "Train Epoch: 1 [27648/50000 (55%)]\tLoss: 1.775510\n",
            "Train Epoch: 1 [28672/50000 (57%)]\tLoss: 1.685771\n",
            "Train Epoch: 1 [29696/50000 (59%)]\tLoss: 1.712522\n",
            "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 1.682399\n",
            "Train Epoch: 1 [31744/50000 (63%)]\tLoss: 1.836319\n",
            "Train Epoch: 1 [32768/50000 (65%)]\tLoss: 1.698110\n",
            "Train Epoch: 1 [33792/50000 (68%)]\tLoss: 1.537227\n",
            "Train Epoch: 1 [34816/50000 (70%)]\tLoss: 1.289982\n",
            "Train Epoch: 1 [35840/50000 (72%)]\tLoss: 1.765829\n",
            "Train Epoch: 1 [36864/50000 (74%)]\tLoss: 1.554089\n",
            "Train Epoch: 1 [37888/50000 (76%)]\tLoss: 1.553251\n",
            "Train Epoch: 1 [38912/50000 (78%)]\tLoss: 1.316247\n",
            "Train Epoch: 1 [39936/50000 (80%)]\tLoss: 1.411522\n",
            "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 1.441667\n",
            "Train Epoch: 1 [41984/50000 (84%)]\tLoss: 1.399782\n",
            "Train Epoch: 1 [43008/50000 (86%)]\tLoss: 1.809432\n",
            "Train Epoch: 1 [44032/50000 (88%)]\tLoss: 1.522408\n",
            "Train Epoch: 1 [45056/50000 (90%)]\tLoss: 1.602210\n",
            "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 1.643748\n",
            "Train Epoch: 1 [47104/50000 (94%)]\tLoss: 1.360403\n",
            "Train Epoch: 1 [48128/50000 (96%)]\tLoss: 1.553722\n",
            "Train Epoch: 1 [49152/50000 (98%)]\tLoss: 1.589059\n",
            "\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4533/10000 (45%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.325136\n",
            "Train Epoch: 2 [1024/50000 (2%)]\tLoss: 1.410840\n",
            "Train Epoch: 2 [2048/50000 (4%)]\tLoss: 1.556014\n",
            "Train Epoch: 2 [3072/50000 (6%)]\tLoss: 1.546016\n",
            "Train Epoch: 2 [4096/50000 (8%)]\tLoss: 1.417047\n",
            "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 1.184316\n",
            "Train Epoch: 2 [6144/50000 (12%)]\tLoss: 1.572073\n",
            "Train Epoch: 2 [7168/50000 (14%)]\tLoss: 1.613960\n",
            "Train Epoch: 2 [8192/50000 (16%)]\tLoss: 1.290747\n",
            "Train Epoch: 2 [9216/50000 (18%)]\tLoss: 1.479607\n",
            "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 1.427565\n",
            "Train Epoch: 2 [11264/50000 (23%)]\tLoss: 1.410524\n",
            "Train Epoch: 2 [12288/50000 (25%)]\tLoss: 1.200438\n",
            "Train Epoch: 2 [13312/50000 (27%)]\tLoss: 1.394696\n",
            "Train Epoch: 2 [14336/50000 (29%)]\tLoss: 1.352689\n",
            "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 1.238443\n",
            "Train Epoch: 2 [16384/50000 (33%)]\tLoss: 1.437500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6030a3930da9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;31m# writer.add_scalar('Train/Loss', loss.item(), (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-6030a3930da9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./optimizer.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;31m# .cpu() on the underlying Storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/storage.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;34m\"\"\"Returns a CPU copy of this storage if it's not already on the CPU\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;31m# or on typing_extensions module on Python >= 3.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0m__new__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, dtype, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}